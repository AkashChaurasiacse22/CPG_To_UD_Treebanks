{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiDya0Ji0y9c"
      },
      "outputs": [],
      "source": [
        "!pip install google-cloud-aiplatform\n",
        "!pip install --user datasets\n",
        "!pip install --user google-cloud-pipeline-components"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "L8CG57N40_kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "from google.cloud import aiplatform\n",
        "from google.colab import auth as google_auth\n",
        "google_auth.authenticate_user()"
      ],
      "metadata": {
        "id": "RkGaTrKY1G-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "PROJECT_ID = \"YOUR_PROJECT_ID\"\n",
        "vertexai.init(project=PROJECT_ID)"
      ],
      "metadata": {
        "id": "6qfQ12ss1Jkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "region = \"us-central1\"\n",
        "REGION = \"us-central1\"\n",
        "project_id = \"YOUR_PROJECT_ID\""
      ],
      "metadata": {
        "id": "SZC5RTJs1NPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! gcloud config set project {project_id}"
      ],
      "metadata": {
        "id": "M-HX_w7m1QCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import vertexai\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)\n",
        "import kfp\n",
        "import sys\n",
        "import uuid\n",
        "import json\n",
        "import vertexai\n",
        "import pandas as pd\n",
        "from google.auth import default\n",
        "from datasets import load_dataset\n",
        "from google.cloud import aiplatform\n",
        "from vertexai.preview.language_models import TextGenerationModel, EvaluationTextSummarizationSpec"
      ],
      "metadata": {
        "id": "Z2WATN1K1X14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUCKET_NAME = 'YOUR_BUCKET'\n",
        "BUCKET_URI = f\"gs://YOUR_BUCKET/TRAIN.jsonl\"\n",
        "REGION = \"us-central1\""
      ],
      "metadata": {
        "id": "CplEVPPU1Y7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we have to use it with the colab repository"
      ],
      "metadata": {
        "id": "SXB1p1Bo3Kdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_json(\"/content/drive/MyDrive/Training_Data_BTP/train.jsonl\", lines=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "k4FFyySk2Q9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we have to use the data which is there in bucket then we can do this by this code\n"
      ],
      "metadata": {
        "id": "1YKoiDuY3PlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "json_url = 'https://storage.googleapis.com/YOUR_BUCKET/TRAIN.jsonl'\n",
        "df = pd.read_json(json_url, lines=True)\n",
        "print (df)"
      ],
      "metadata": {
        "id": "j06GhSQp3aRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)"
      ],
      "metadata": {
        "id": "IxCcsW8l2xTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_display_name = 'cpg_to_ud_finetunned_model'\n",
        "tuned_model = TextGenerationModel.from_pretrained(\"gemini-2.0-flash-lite-001\")\n",
        "tuned_model.tune_model(\n",
        "training_data=df,\n",
        "train_steps=100,\n",
        "tuning_job_location=\"us-central1\",\n",
        "tuned_model_location=\"us-central1\",\n",
        ")"
      ],
      "metadata": {
        "id": "XZ3WMw7f2zJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_model_name = tuned_model._endpoint.gca_resource.deployed_models[0].model\n",
        "tuned_model_1 = TextGenerationModel.get_tuned_model(tuned_model_name)\n",
        "response = tuned_model_1.predict(\"\"\"<Sentence id='1'>\n",
        "1\t((\tNP\t<fs name='NP' drel='r6:NP2'>\n",
        "1.1\tकरीब\tRP_RPD\t<fs af='करीब,avy,,,,,,' name='करीब' posn='10'>\n",
        "1.2\t40,000\tQT_QTC\t<fs af='40000,num,any,any,,any,,' name='40,000' posn='20'>\n",
        "1.3\tअंडों\tN_NN\t<fs af='अंड,n,m,pl,3,o,0,0' name='अंडों' posn='30'>\n",
        "1.4\tका\tPSP\t<fs af='का,psp,m,sg,,d,,' name='का' posn='40'>\n",
        "\t))\n",
        "2\t((\tNP\t<fs name='NP2' drel='k1:VGF'>\n",
        "2.1\tवजन\tN_NN\t<fs af='वजन,n,m,sg,3,d,0,0' name='वजन' posn='50'>\n",
        "\t))\n",
        "3\t((\tNP\t<fs name='NP3' drel='k1u:VGF'>\n",
        "3.1\tकेवल\tRP_RPD\t<fs af='केवल,avy,,,,,,' name='केवल' posn='60'>\n",
        "3.2\tएक\tQT_QTC\t<fs af='एक,num,any,any,,any,,' name='एक' posn='70'>\n",
        "3.3\tआउंस\tN_NN\t<fs af='आउंस,n,m,sg,3,o,0,0' name='आउंस' posn='80'>\n",
        "3.4\tके\tPSP\t<fs af='के,psp,,,,,,' name='के' posn='90'>\n",
        "3.5\tलगभग\tRP_RPD\t<fs af='लगभग,avy,,,,,,' name='लगभग' posn='100'>\n",
        "\t))\n",
        "4\t((\tVGF\t<fs name='VGF' voicetype='active' stype='declarative'>\n",
        "4.1\tहोता\tV_VM\t<fs af='हो,v,m,sg,any,,ता,wA' name='होता' posn='110'>\n",
        "4.2\tहै\tV_VAUX\t<fs af='है,v,any,sg,3,,0,0' name='है' posn='120'>\n",
        "\t))\n",
        "5\t((\tBLK\t<fs name='BLK' drel='rsym:VGF'>\n",
        "5.1\t।\tRD_PUNC\t<fs af='।,punc,,,,,,' name='।' posn='130'>\n",
        "\t))\n",
        "</Sentence>\"\"\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "nMgTCrrq30M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Predictions for CPG Files into UD files."
      ],
      "metadata": {
        "id": "uuuPL-5TFG0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "\n",
        "def get_cpg_files_from_directory(root_dir):\n",
        "    cpg_files = []\n",
        "    for root, _, files in os.walk(root_dir):\n",
        "        for file in files:\n",
        "            cpg_files.append(os.path.join(root, file))\n",
        "    return cpg_files\n",
        "def parse_cpg_blocks(cpg_files):\n",
        "    all_blocks = []\n",
        "    for cpg_path in cpg_files:\n",
        "        with open(cpg_path, 'r', encoding='utf-8') as f:\n",
        "            block = []\n",
        "            for line in f:\n",
        "                stripped = line.rstrip()\n",
        "                if \"<Sentence id=\" in stripped:\n",
        "                    block = [stripped]\n",
        "                elif \"</Sentence>\" in stripped:\n",
        "                    block.append(stripped)\n",
        "                    # join lines into one block\n",
        "                    all_blocks.append(\"\\n\".join(block))\n",
        "                    block = []\n",
        "                elif block:\n",
        "                    block.append(stripped)\n",
        "    return all_blocks\n",
        "\n",
        "cpg_root_directory = \"/content/drive/MyDrive/Training_Data_BTP/HINDI-DEPENDENCY-ALL-DOMAINS-LATEST/Data/\"\n",
        "cpg_files = get_cpg_files_from_directory(cpg_root_directory)\n",
        "OUTPUT_CONLLU     = \"/content/drive/MyDrive/Training_Data_BTP/predictions.conllu\"\n",
        "\n",
        "cpg_blocks = parse_cpg_blocks(cpg_files)\n",
        "print(f\"Parsed {len(cpg_blocks)} sentences from CPG files.\")\n",
        "\n",
        "with open(OUTPUT_CONLLU, \"w\", encoding=\"utf-8\") as fout:\n",
        "    for idx, cpg in enumerate(cpg_blocks, 1):\n",
        "        prompt = cpg.strip() + \"\\n\\n\"\n",
        "\n",
        "        response = tuned_model_1.predict(prompt)\n",
        "        ud_conllu = response.text.strip()\n",
        "\n",
        "        fout.write(ud_conllu.rstrip() + \"\\n\\n\")\n",
        "\n",
        "        if idx % 50 == 0:\n",
        "            print(f\"  • Processed {idx}/{len(cpg_blocks)} sentences\")\n",
        "\n",
        "print(f\"\\n All done — UD output in: {OUTPUT_CONLLU}\")\n"
      ],
      "metadata": {
        "id": "qdS5u9TDBESK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}